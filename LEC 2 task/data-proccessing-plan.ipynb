{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3bbff2",
   "metadata": {},
   "source": [
    "\n",
    "### **Data Preprocessing & Feature Engineering Strategy**\n",
    "\n",
    "**Objective:** To systematically clean, transform, and engineer the dataset to create a robust set of features for predictive modeling, while mitigating risks like data leakage and the curse of dimensionality.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Phase 1: Numerical Feature Processing üìä**\n",
    "\n",
    "This phase focuses on refining the numerical data to be suitable for modeling.\n",
    "\n",
    "**1.1. Feature Removal & Selection:**\n",
    "\n",
    "  * **Action:** Drop columns that do not carry predictive value or are redundant.\n",
    "  * **Columns Dropped:**\n",
    "      * `Id`: A non-predictive identifier.\n",
    "      * `MoSold`, `YrSold`: Sales dates, often less predictive than age-related features.\n",
    "      * `YearRemodAdd`: To reduce redundancy with `YearBuilt`, as planned.\n",
    "  * **Regarding Multicollinearity:**\n",
    "      * You dropped `TotalBsmtSF`, `GarageArea`, `1stFlrSF`, and `GarageYrBlt`. This is a strong step.\n",
    "      * **Professional Note:** Before dropping, it's standard practice to confirm high correlation using a heatmap or by calculating the Variance Inflation Factor (VIF). Dropping `1stFlrSF` and `TotalBsmtSF` is aggressive as they are powerful predictors; ensure this decision is well-supported by your analysis.\n",
    "\n",
    "**1.2. Missing Value Imputation:**\n",
    "\n",
    "  * **Strategy:** Impute missing numerical values using a robust central tendency metric.\n",
    "  * **Action:**\n",
    "      * `LotFrontage`: Fill `NaN` values with the **median** of the column. The median is generally preferred over the mean as it's less sensitive to outliers.\n",
    "    <!-- end list -->\n",
    "    ```python\n",
    "    df['LotFrontage'] = df['LotFrontage'].fillna(df['LotFrontage'].median())\n",
    "    ```\n",
    "\n",
    "**1.3. Skewness Transformation:**\n",
    "\n",
    "  * **Strategy:** Normalize the distribution of highly skewed features to improve the performance of linear models and stabilize variance.\n",
    "  * **Action:** Apply a log transformation (`np.log1p`) to skewed continuous variables during the feature engineering phase.\n",
    "    ```python\n",
    "    df['MiscVal_log'] = np.log1p(df['MiscVal'])\n",
    "    ```\n",
    "\n",
    "-----\n",
    "\n",
    "### **Phase 2: Categorical Feature Processing üè∑Ô∏è**\n",
    "\n",
    "This phase handles all non-numeric data by converting it into a machine-readable format, paying close attention to the inherent nature of the data.\n",
    "\n",
    "**2.1. Unified Missing Value Strategy:**\n",
    "\n",
    "  * **Strategy:** Treat `NaN` values in categorical columns as a distinct category representing the \"absence\" of a feature.\n",
    "  * **Action:** Fill `NaN` values with the string `'None'`. This is more explicit and safer than using `0`, which could be misinterpreted as an ordinal value.\n",
    "    ```python\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].fillna('None')\n",
    "    ```\n",
    "\n",
    "**2.2. Ordinal Feature Encoding:**\n",
    "\n",
    "  * **Strategy:** Manually map ordinal features to numerical values to preserve their inherent rank and logical order without increasing dimensionality.\n",
    "  * **Columns:** `['ExterQual', 'ExterCond', 'BsmtQual', ...]`\n",
    "  * **Action:** Define a clear mapping for each feature and apply it using the `.map()` method.\n",
    "    ```python\n",
    "    # Example for one column\n",
    "    quality_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0}\n",
    "    df['ExterQual_encoded'] = df['ExterQual'].map(quality_map)\n",
    "    ```\n",
    "\n",
    "**2.3. Nominal Feature Encoding:**\n",
    "\n",
    "  * **Strategy:** Convert nominal (non-ordered) features into a numerical format using One-Hot Encoding, creating new binary columns for each category.\n",
    "  * **Action:** Use `pandas.get_dummies()` to perform the encoding. To prevent perfect multicollinearity, it's standard practice to drop one of the new dummy columns for each feature.\n",
    "    ```python\n",
    "    # drop_first=True is the key professional step\n",
    "    df = pd.get_dummies(df, columns=nominal_cols, drop_first=True)\n",
    "    ```\n",
    "\n",
    "-----\n",
    "\n",
    "### **Phase 3: Finalization & Validation üèÅ**\n",
    "\n",
    "This final phase ensures the data is ready for the modeling pipeline.\n",
    "\n",
    "**3.1. Data Splitting:**\n",
    "\n",
    "  * **Action:** Split the fully processed data into training and testing sets **before** applying feature scaling. This prevents data leakage from the test set into the training process.\n",
    "    ```python\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    ```\n",
    "\n",
    "**3.2. Feature Scaling:**\n",
    "\n",
    "  * **Action:** Apply a scaler (e.g., `StandardScaler` or `MinMaxScaler`) to all numerical features (including your newly encoded ordinal features) to ensure they are on a comparable scale.\n",
    "    ```python\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    ```\n",
    "\n",
    "This structured plan not only organizes your existing ideas but also incorporates professional best practices like using the median, `drop_first=True`, and the critical final steps of splitting and scaling."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
